---
title: "ADS 503 - Team 7"
author: "Summer Purschke, Jacqueline Urenda, Oscar Gil"
date: "06/12/2022"
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE,message=FALSE}
# R Libraries
library(caret)
library(AppliedPredictiveModeling)
library(Hmisc)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(corrplot)
library(MASS)
library(ISLR)
library(rpart)
library(partykit)
library(randomForestSRC)
library(earth)
library(MARSS)
library(e1071)
```

## Pre-processing
## Load the Red Wine Quality data set from GitHub - data set copied from Kaggle and imported into GitHub.
```{r, warning=FALSE,message=FALSE, fig.height= 4, fig.width= 6}
wine <- read.csv(
  url("https://raw.githubusercontent.com/OscarG-DataSci/ADS503/main/winequality-red.csv")
                      , header = TRUE)

# No missing data
summary(wine)

# Quality variable distribution count
wine %>% count(quality)
histogram(wine$quality)

# Create new variable, for quality values, split by half (0, 1)
wine$quality_target <- ifelse( wine$quality <= 5, 0, 1)

# New variable, distribution counts
wine %>% count(quality_target)

# Mean of new variable is at 0.5347 (close enough to 50% to maintain balance)
summary(wine$quality_target)

# Check for missing values in data set
wine %>% na.omit() %>% count() # there are no missing values


# Correlation Matrix
cor <- cor(wine)

# Colors for Correlation Matrix
colors <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(cor, order="hclust", method = "color", addCoef.col = "black"
         , tl.srt = 45, number.cex = 0.47, col=colors(200))


# Cutoff Correlation features
cutoffCorr <- findCorrelation(cor, cutoff = .8)
cutoffCorrFeatures <- wine[, -cutoffCorr]
```
```
```
## Logistic Regression Model
```{r, warning=FALSE,message=FALSE}
# Cutoff Correlation string for lm.fit
subset(cutoffCorrFeatures, select = -c(quality_target)) %>% 
      colnames() %>% 
      paste0(collapse = " + ")

# Train and Test split
wine_split <- createDataPartition(wine$quality, p = .8, list = FALSE)
wine_train <- wine[ wine_split,]
wine_test  <- wine[-wine_split,]


# Model
lmodel <- lm(quality~ volatile.acidity + sulphates + alcohol, data = wine_train)

summary(lmodel)

# Add predicted values to new data frame
wine_test %>%
  mutate(predicted = predict(lmodel, newdata = wine_test)) -> df

# Summary of predicted interval
predict(lmodel, newdata = wine_test, interval = "prediction") %>%
  summary()

# First few rows of actual data with predictions
subset(df, select = c(volatile.acidity, total.sulfur.dioxide, 
               sulphates, alcohol, quality_target, quality, predicted)) %>%
  head(3)


# Scatter plot of predicted 
ggplot(df, aes(x = predicted, y = quality, colour = quality ))+
geom_point(alpha = 0.3, position = position_jitter()) + stat_smooth()

# The scatter plot supports the summary of the predicted interval, in the ranges of the fit, 
# lower, and upper ranges. The R-squared value of 0.3495 of the model, indicates that this 
# information can be predicted 35% of the time, with the data available, for the variance 
# of the information.

```
```
```
## CART
```{r, warning=FALSE,message=FALSE}
# Subset both train and test sets, to excluse "quality_target"
subset(wine_train, select = -c(quality_target)) -> rf_wine_train
subset(wine_test, select = -c(quality_target)) -> rf_wine_test
  
rPartTree <- rpart(quality ~ ., data = rf_wine_train)

rpartTree2 <- as.party(rPartTree)

# Results
rpartTree2

plot(rpartTree2, gp = gpar(fontsize=6))

```
```
```
## Random Forest
```{r, warning=FALSE,message=FALSE}
rf <- rfsrc(quality ~ ., data = rf_wine_train)

print(rf)
  
# Variable Importance
vi <- subsample(rf, verbose = FALSE)

extract.subsample(vi)$var.jk.sel.Z

# Variable Importance Plot
plot(vi)

# Predict
# https://www.rdocumentation.org/packages/randomForestSRC/versions/3.1.0/topics/predict.rfsrc
randomForestSRC::predict.rfsrc(rf, rf_wine_test)
```
```
```
## Partial Least Squares
```{r, warning=FALSE,message=FALSE}
tctrl <- trainControl(method = "repeatedcv", repeats = 5, number =10)

set.seed(4)
pls_wine <- train(quality~ volatile.acidity + chlorides + total.sulfur.dioxide + 
               sulphates + alcohol, data = wine_train,
                  method = "pls",
                  preProc = c("center", "scale", "BoxCox"),
                  tunelength =20,
                  trControl = tctrl)

pls_wine
```
```
```
## Mars Tuning
```{r, warning=FALSE,message=FALSE}
mars_wine <- earth(quality~ volatile.acidity + chlorides + total.sulfur.dioxide + 
               sulphates + alcohol, data =wine_train)

mars_wine

summary(mars_wine)

preProc_Arguments = c("center", "scale")
marsGrid_wine = expand.grid(.degree=1:2, .nprune=2:38)

set.seed(0)

marsModel_wine = train(quality~ volatile.acidity + chlorides + total.sulfur.dioxide + 
                       sulphates + alcohol, data =wine_train, 
                       method="earth", 
                       preProc=preProc_Arguments, 
                       tuneGrid=marsGrid_wine)

marsModel_wine
```
```
```
## KNN Neighbors
```{r, warning=FALSE,message=FALSE}
set.seed(50)

knn_wine <- train(quality~ volatile.acidity + chlorides + total.sulfur.dioxide + 
               sulphates + alcohol, data =wine_train,
               method = "knn",
               preProc = c("center", "scale"),
               tuneGrid = data.frame(.k = 1:50),
               trControl = trainControl(method = "cv")) 

knn_wine
```
```
```
## SVM
```{r, warning=FALSE,message=FALSE}
set.seed(47)

subset(wine_train, select = -c(quality_target, quality)) -> predictors
wine_train$quality -> quality

svmTune <- train(predictors, quality,
                 method = "svmRadial",
                 preProc = c("center", "scale"),
                 tuneLength= 5,
                 trControl = trainControl(method = "cv"))
svmTune
```
```
```
## Penalized Logistic Regression Tuning
```{r, warning=FALSE,message=FALSE}
#tuning parameters, alpha is associated with the ridge(0) versus lasso regression(1)
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 5))

glmnTune <- train(x = predictors, 
                 y = quality,
                 method = "glmnet",
                 tuneGrid = glmnGrid,
                 preProc = c("center", "scale"),
                 trControl = trainControl(method = "cv"))

glmnTune
```